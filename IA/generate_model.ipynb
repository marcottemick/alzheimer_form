{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('alzheimers_disease_data.csv', sep=',')\n",
    "df = df.drop(columns=['PatientID', 'DoctorInCharge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = df[df.Diagnosis == 0]\n",
    "df_minority = df[df.Diagnosis == 1]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,    # Échantillonnage sans remplacement\n",
    "                                   n_samples=len(df_minority),     # Pour obtenir le même nombre de samples que la classe minoritaire\n",
    "                                   random_state=0) # Pour reproduire les résultats\n",
    "\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compter les occurrences de chaque valeur\n",
    "value_counts = df_balanced['Diagnosis'].value_counts()\n",
    "\n",
    "# Créer le graphique à barres\n",
    "plt.figure(figsize=(8, 6))\n",
    "value_counts.plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Répartition des diagnostics après rééquilibrage des labels')\n",
    "plt.xlabel('Valeur')\n",
    "plt.ylabel('Nombre d\\'occurrences')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les pipelines pour différents modèles\n",
    "pipelines = {\n",
    "    'logistic_regression_standard': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ]),\n",
    "    'logistic_regression_minmax': Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ]),\n",
    "    'decision_tree': Pipeline([\n",
    "        ('classifier', DecisionTreeClassifier())\n",
    "    ]),\n",
    "    'random_forest': Pipeline([\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ]),\n",
    "    'gradient_boosting': Pipeline([\n",
    "        ('classifier', GradientBoostingClassifier())\n",
    "    ]),\n",
    "    'svm_standard': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', SVC())\n",
    "    ]),\n",
    "    'svm_minmax': Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('classifier', SVC())\n",
    "    ]),\n",
    "    'knn_standard': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ]),\n",
    "    'knn_minmax': Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ]),\n",
    "    'naive_bayes_standard': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', GaussianNB())\n",
    "    ]),\n",
    "    'naive_bayes_minmax': Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('classifier', GaussianNB())\n",
    "    ]),\n",
    "}\n",
    "\n",
    "'''\n",
    "Best pipeline: DecisionTreeClassifier(input_matrix, criterion=entropy, max_depth=5, min_samples_leaf=8, min_samples_split=2)\n",
    "'''\n",
    "\n",
    "# Définir les grilles de paramètres pour chaque modèle\n",
    "param_grids = {\n",
    "    'logistic_regression_standard': {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'elasticnet', 'l2'],\n",
    "        'classifier__solver': ['lbfgs', 'saga'],\n",
    "        'classifier__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'logistic_regression_minmax': {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__penalty': ['l1', 'elasticnet', 'l2'],\n",
    "        'classifier__solver': ['lbfgs', 'saga'],\n",
    "        'classifier__max_iter': [100, 200, 300]\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'classifier__max_depth': [5, 10, 20, 30, 40],\n",
    "        'classifier__min_samples_split': [2, 5, 10, 20],\n",
    "        'classifier__min_samples_leaf': [1, 2, 5],\n",
    "        'classifier__criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'classifier__n_estimators': [50, 100, 200, 500],\n",
    "        'classifier__max_depth': [5, 10, 20, 30, 40],\n",
    "        'classifier__min_samples_split': [2, 5, 10, 20],\n",
    "        'classifier__min_samples_leaf': [1, 2, 5],\n",
    "        'classifier__bootstrap': [True, False]\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'classifier__n_estimators': [50, 100, 200, 500],\n",
    "        'classifier__learning_rate': [0.001, 0.01, 0.1, 0.5],\n",
    "        'classifier__max_depth': [3, 5, 7, 10],\n",
    "        'classifier__subsample': [0.6, 0.8, 1.0]\n",
    "    },\n",
    "    'svm_standard': {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'classifier__gamma': ['scale', 'auto'],\n",
    "        'classifier__degree': [3, 4, 5]  # applicable for 'poly' kernel\n",
    "    },\n",
    "    'svm_minmax': {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'classifier__gamma': ['scale', 'auto'],\n",
    "        'classifier__degree': [3, 4, 5]\n",
    "    },\n",
    "    'knn_standard': {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__p': [1, 2]  # p=1 for Manhattan, p=2 for Euclidean distance\n",
    "    },\n",
    "    'knn_minmax': {\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__p': [1, 2]\n",
    "    },\n",
    "    'naive_bayes_standard': {\n",
    "        'classifier__var_smoothing': [1e-10, 1e-09, 1e-08, 1e-07]\n",
    "    },\n",
    "    'naive_bayes_minmax': {\n",
    "        'classifier__var_smoothing': [1e-10, 1e-09, 1e-08, 1e-07]\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def generetate_models(X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame, folder: str):\n",
    "    # Effectuer la recherche par grille pour chaque modèle\n",
    "    best_models = {}\n",
    "    for model_name in pipelines:\n",
    "        print(f'Optimizing {model_name}...')\n",
    "        grid_search = GridSearchCV(pipelines[model_name], param_grids[model_name], cv=5, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "        print(f'Best parameters for {model_name}: {grid_search.best_params_}')\n",
    "        print(f'Best score for {model_name}: {grid_search.best_score_}')\n",
    "\n",
    "    # Évaluer les meilleurs modèles sur le set de test\n",
    "    for model_name, model in best_models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f'\\nModel: {model_name}')\n",
    "        joblib.dump(model, '{}/{}.joblib'.format(folder, model_name))\n",
    "        print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "        print('Classification Report:')\n",
    "        print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing logistic_regression_standard...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.79112191        nan        nan 0.81414693 0.81414693\n",
      "        nan 0.79112191        nan        nan 0.81414693 0.81414693\n",
      "        nan 0.79112191        nan        nan 0.81414693 0.81414693\n",
      "        nan 0.82975781        nan        nan 0.81332726 0.81332726\n",
      "        nan 0.82975781        nan        nan 0.81332726 0.81332726\n",
      "        nan 0.82975781        nan        nan 0.81332726 0.81332726\n",
      "        nan 0.81414019        nan        nan 0.81414693 0.81414693\n",
      "        nan 0.81414019        nan        nan 0.81414693 0.81414693\n",
      "        nan 0.81414019        nan        nan 0.81414693 0.81414693\n",
      "        nan 0.81578965        nan        nan 0.81496998 0.81496998\n",
      "        nan 0.81578965        nan        nan 0.81496998 0.81496998\n",
      "        nan 0.81578965        nan        nan 0.81496998 0.81496998\n",
      "        nan 0.81496998        nan        nan 0.81496998 0.81496998\n",
      "        nan 0.81496998        nan        nan 0.81496998 0.81496998\n",
      "        nan 0.81496998        nan        nan 0.81496998 0.81496998]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for logistic_regression_standard: {'classifier__C': 0.1, 'classifier__max_iter': 100, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
      "Best score for logistic_regression_standard: 0.8297578088106322\n",
      "Optimizing logistic_regression_minmax...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.50904675        nan        nan 0.80757269 0.80757269\n",
      "        nan 0.50904675        nan        nan 0.80757269 0.80757269\n",
      "        nan 0.50904675        nan        nan 0.80757269 0.80757269\n",
      "        nan 0.8363388         nan        nan 0.8157829  0.8157829\n",
      "        nan 0.8363388         nan        nan 0.8157829  0.8157829\n",
      "        nan 0.8363388         nan        nan 0.8157829  0.8157829\n",
      "        nan 0.82072118        nan        nan 0.81249747 0.8116778\n",
      "        nan 0.82072118        nan        nan 0.81249747 0.8116778\n",
      "        nan 0.82072118        nan        nan 0.81249747 0.8116778\n",
      "        nan 0.8166127         nan        nan 0.81414693 0.81496998\n",
      "        nan 0.8166127         nan        nan 0.81414693 0.81496998\n",
      "        nan 0.8166127         nan        nan 0.81414693 0.81496998\n",
      "        nan 0.81496998        nan        nan 0.81496998 0.81496998\n",
      "        nan 0.81578965        nan        nan 0.81496998 0.81496998\n",
      "        nan 0.81496998        nan        nan 0.81496998 0.81496998]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for logistic_regression_minmax: {'classifier__C': 0.1, 'classifier__max_iter': 100, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
      "Best score for logistic_regression_minmax: 0.8363387978142077\n",
      "Optimizing decision_tree...\n",
      "Best parameters for decision_tree: {'classifier__criterion': 'entropy', 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2}\n",
      "Best score for decision_tree: 0.9317547055251973\n",
      "Optimizing random_forest...\n",
      "Best parameters for random_forest: {'classifier__bootstrap': False, 'classifier__max_depth': 30, 'classifier__min_samples_leaf': 5, 'classifier__min_samples_split': 20, 'classifier__n_estimators': 200}\n",
      "Best score for random_forest: 0.9416245024623896\n",
      "Optimizing gradient_boosting...\n",
      "Best parameters for gradient_boosting: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__n_estimators': 200, 'classifier__subsample': 0.6}\n",
      "Best score for gradient_boosting: 0.9391553666599204\n",
      "Optimizing svm_standard...\n",
      "Best parameters for svm_standard: {'classifier__C': 1, 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear'}\n",
      "Best score for svm_standard: 0.8297679282196586\n",
      "Optimizing svm_minmax...\n",
      "Best parameters for svm_minmax: {'classifier__C': 10, 'classifier__degree': 3, 'classifier__gamma': 'auto', 'classifier__kernel': 'rbf'}\n",
      "Best score for svm_minmax: 0.8305842272144641\n",
      "Optimizing knn_standard...\n",
      "Best parameters for knn_standard: {'classifier__n_neighbors': 9, 'classifier__p': 1, 'classifier__weights': 'uniform'}\n",
      "Best score for knn_standard: 0.7236861633947245\n",
      "Optimizing knn_minmax...\n",
      "Best parameters for knn_minmax: {'classifier__n_neighbors': 9, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n",
      "Best score for knn_minmax: 0.7006881198138029\n",
      "Optimizing naive_bayes_standard...\n",
      "Best parameters for naive_bayes_standard: {'classifier__var_smoothing': 1e-10}\n",
      "Best score for naive_bayes_standard: 0.8010018214936249\n",
      "Optimizing naive_bayes_minmax...\n",
      "Best parameters for naive_bayes_minmax: {'classifier__var_smoothing': 1e-10}\n",
      "Best score for naive_bayes_minmax: 0.8010018214936249\n",
      "\n",
      "Model: logistic_regression_standard\n",
      "Accuracy: 0.8256578947368421\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.83       163\n",
      "           1       0.80      0.83      0.82       141\n",
      "\n",
      "    accuracy                           0.83       304\n",
      "   macro avg       0.82      0.83      0.83       304\n",
      "weighted avg       0.83      0.83      0.83       304\n",
      "\n",
      "\n",
      "Model: logistic_regression_minmax\n",
      "Accuracy: 0.8157894736842105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83       163\n",
      "           1       0.80      0.81      0.80       141\n",
      "\n",
      "    accuracy                           0.82       304\n",
      "   macro avg       0.81      0.82      0.81       304\n",
      "weighted avg       0.82      0.82      0.82       304\n",
      "\n",
      "\n",
      "Model: decision_tree\n",
      "Accuracy: 0.9276315789473685\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93       163\n",
      "           1       0.93      0.91      0.92       141\n",
      "\n",
      "    accuracy                           0.93       304\n",
      "   macro avg       0.93      0.93      0.93       304\n",
      "weighted avg       0.93      0.93      0.93       304\n",
      "\n",
      "\n",
      "Model: random_forest\n",
      "Accuracy: 0.9407894736842105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.95       163\n",
      "           1       0.96      0.91      0.93       141\n",
      "\n",
      "    accuracy                           0.94       304\n",
      "   macro avg       0.94      0.94      0.94       304\n",
      "weighted avg       0.94      0.94      0.94       304\n",
      "\n",
      "\n",
      "Model: gradient_boosting\n",
      "Accuracy: 0.9342105263157895\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       163\n",
      "           1       0.95      0.91      0.93       141\n",
      "\n",
      "    accuracy                           0.93       304\n",
      "   macro avg       0.94      0.93      0.93       304\n",
      "weighted avg       0.93      0.93      0.93       304\n",
      "\n",
      "\n",
      "Model: svm_standard\n",
      "Accuracy: 0.8157894736842105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       163\n",
      "           1       0.80      0.80      0.80       141\n",
      "\n",
      "    accuracy                           0.82       304\n",
      "   macro avg       0.81      0.81      0.81       304\n",
      "weighted avg       0.82      0.82      0.82       304\n",
      "\n",
      "\n",
      "Model: svm_minmax\n",
      "Accuracy: 0.805921052631579\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82       163\n",
      "           1       0.79      0.79      0.79       141\n",
      "\n",
      "    accuracy                           0.81       304\n",
      "   macro avg       0.80      0.81      0.80       304\n",
      "weighted avg       0.81      0.81      0.81       304\n",
      "\n",
      "\n",
      "Model: knn_standard\n",
      "Accuracy: 0.7401315789473685\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.77       163\n",
      "           1       0.75      0.66      0.70       141\n",
      "\n",
      "    accuracy                           0.74       304\n",
      "   macro avg       0.74      0.73      0.74       304\n",
      "weighted avg       0.74      0.74      0.74       304\n",
      "\n",
      "\n",
      "Model: knn_minmax\n",
      "Accuracy: 0.7269736842105263\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.79      0.76       163\n",
      "           1       0.73      0.66      0.69       141\n",
      "\n",
      "    accuracy                           0.73       304\n",
      "   macro avg       0.73      0.72      0.72       304\n",
      "weighted avg       0.73      0.73      0.73       304\n",
      "\n",
      "\n",
      "Model: naive_bayes_standard\n",
      "Accuracy: 0.7894736842105263\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.80       163\n",
      "           1       0.75      0.81      0.78       141\n",
      "\n",
      "    accuracy                           0.79       304\n",
      "   macro avg       0.79      0.79      0.79       304\n",
      "weighted avg       0.79      0.79      0.79       304\n",
      "\n",
      "\n",
      "Model: naive_bayes_minmax\n",
      "Accuracy: 0.7894736842105263\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.77      0.80       163\n",
      "           1       0.75      0.81      0.78       141\n",
      "\n",
      "    accuracy                           0.79       304\n",
      "   macro avg       0.79      0.79      0.79       304\n",
      "weighted avg       0.79      0.79      0.79       304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df_balanced.Diagnosis\n",
    "X = df_balanced.drop(columns=['Diagnosis'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "generetate_models(X_train, y_train, X_test, y_test, 'all_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(road_model: str, X_test:pd.DataFrame, y_test:pd.DataFrame, row: int):\n",
    "    loaded_model = joblib.load(road_model)\n",
    "\n",
    "    y_new_pred = loaded_model.predict_proba(X_test[row:row+1])\n",
    "    confidence_scores = y_new_pred.max(axis=1)\n",
    "\n",
    "    print(\"données soumises\", X_test[row:row+1].to_dict(orient='records'))\n",
    "    print(\"réponse attendue\", y_test[row:row+1].values.tolist()[0])\n",
    "    print(\"réponse prédicte\", y_new_pred[0])\n",
    "\n",
    "    for i, (pred, confidence) in enumerate(zip(y_new_pred, confidence_scores)):\n",
    "        print(f'Prédiction: {pred}, Confiance: {confidence:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "données soumises [{'Age': 76, 'Gender': 1, 'Ethnicity': 2, 'EducationLevel': 0, 'BMI': 33.8293639108859, 'Smoking': 0, 'AlcoholConsumption': 19.82631309137609, 'PhysicalActivity': 9.399462708710182, 'DietQuality': 4.794106212456812, 'SleepQuality': 4.976045740143789, 'FamilyHistoryAlzheimers': 0, 'CardiovascularDisease': 1, 'Diabetes': 1, 'Depression': 1, 'HeadInjury': 0, 'Hypertension': 0, 'SystolicBP': 125, 'DiastolicBP': 108, 'CholesterolTotal': 270.5244657436494, 'CholesterolLDL': 113.19711983229844, 'CholesterolHDL': 35.326907803137615, 'CholesterolTriglycerides': 152.87039903395123, 'MMSE': 18.10555514327309, 'FunctionalAssessment': 4.801435992718286, 'MemoryComplaints': 0, 'BehavioralProblems': 0, 'ADL': 2.718906931446053, 'Confusion': 0, 'Disorientation': 0, 'PersonalityChanges': 0, 'DifficultyCompletingTasks': 1, 'Forgetfulness': 1}]\n",
      "réponse attendue 1\n",
      "réponse prédicte [0.29272434 0.70727566]\n",
      "Prédiction: [0.29272434 0.70727566], Confiance: 70.73%\n"
     ]
    }
   ],
   "source": [
    "predict('all_columns/random_forest.joblib', X_test, y_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholConsumption</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>DietQuality</th>\n",
       "      <th>SleepQuality</th>\n",
       "      <th>...</th>\n",
       "      <th>FunctionalAssessment</th>\n",
       "      <th>MemoryComplaints</th>\n",
       "      <th>BehavioralProblems</th>\n",
       "      <th>ADL</th>\n",
       "      <th>Confusion</th>\n",
       "      <th>Disorientation</th>\n",
       "      <th>PersonalityChanges</th>\n",
       "      <th>DifficultyCompletingTasks</th>\n",
       "      <th>Forgetfulness</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22.927749</td>\n",
       "      <td>0</td>\n",
       "      <td>13.297218</td>\n",
       "      <td>6.327112</td>\n",
       "      <td>1.347214</td>\n",
       "      <td>9.025679</td>\n",
       "      <td>...</td>\n",
       "      <td>6.518877</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.725883</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.827681</td>\n",
       "      <td>0</td>\n",
       "      <td>4.542524</td>\n",
       "      <td>7.619885</td>\n",
       "      <td>0.518767</td>\n",
       "      <td>7.151293</td>\n",
       "      <td>...</td>\n",
       "      <td>7.118696</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.592424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17.795882</td>\n",
       "      <td>0</td>\n",
       "      <td>19.555085</td>\n",
       "      <td>7.844988</td>\n",
       "      <td>1.826335</td>\n",
       "      <td>9.673574</td>\n",
       "      <td>...</td>\n",
       "      <td>5.895077</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.119548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.800817</td>\n",
       "      <td>1</td>\n",
       "      <td>12.209266</td>\n",
       "      <td>8.428001</td>\n",
       "      <td>7.435604</td>\n",
       "      <td>8.392554</td>\n",
       "      <td>...</td>\n",
       "      <td>8.965106</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.481226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.716974</td>\n",
       "      <td>0</td>\n",
       "      <td>18.454356</td>\n",
       "      <td>6.310461</td>\n",
       "      <td>0.795498</td>\n",
       "      <td>5.597238</td>\n",
       "      <td>...</td>\n",
       "      <td>6.045039</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39.121757</td>\n",
       "      <td>0</td>\n",
       "      <td>1.561126</td>\n",
       "      <td>4.049964</td>\n",
       "      <td>6.555306</td>\n",
       "      <td>7.535540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.492838</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17.857903</td>\n",
       "      <td>0</td>\n",
       "      <td>18.767261</td>\n",
       "      <td>1.360667</td>\n",
       "      <td>2.904662</td>\n",
       "      <td>8.555256</td>\n",
       "      <td>...</td>\n",
       "      <td>8.687480</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.204952</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.476479</td>\n",
       "      <td>0</td>\n",
       "      <td>4.594670</td>\n",
       "      <td>9.886002</td>\n",
       "      <td>8.120025</td>\n",
       "      <td>5.769464</td>\n",
       "      <td>...</td>\n",
       "      <td>1.972137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.036334</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15.299911</td>\n",
       "      <td>0</td>\n",
       "      <td>8.674505</td>\n",
       "      <td>6.354282</td>\n",
       "      <td>1.263427</td>\n",
       "      <td>8.322874</td>\n",
       "      <td>...</td>\n",
       "      <td>5.173891</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.785399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33.289738</td>\n",
       "      <td>0</td>\n",
       "      <td>7.890703</td>\n",
       "      <td>6.570993</td>\n",
       "      <td>7.941404</td>\n",
       "      <td>9.878711</td>\n",
       "      <td>...</td>\n",
       "      <td>6.307543</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.327563</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2149 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Gender  Ethnicity  EducationLevel        BMI  Smoking  \\\n",
       "0      73       0          0               2  22.927749        0   \n",
       "1      89       0          0               0  26.827681        0   \n",
       "2      73       0          3               1  17.795882        0   \n",
       "3      74       1          0               1  33.800817        1   \n",
       "4      89       0          0               0  20.716974        0   \n",
       "...   ...     ...        ...             ...        ...      ...   \n",
       "2144   61       0          0               1  39.121757        0   \n",
       "2145   75       0          0               2  17.857903        0   \n",
       "2146   77       0          0               1  15.476479        0   \n",
       "2147   78       1          3               1  15.299911        0   \n",
       "2148   72       0          0               2  33.289738        0   \n",
       "\n",
       "      AlcoholConsumption  PhysicalActivity  DietQuality  SleepQuality  ...  \\\n",
       "0              13.297218          6.327112     1.347214      9.025679  ...   \n",
       "1               4.542524          7.619885     0.518767      7.151293  ...   \n",
       "2              19.555085          7.844988     1.826335      9.673574  ...   \n",
       "3              12.209266          8.428001     7.435604      8.392554  ...   \n",
       "4              18.454356          6.310461     0.795498      5.597238  ...   \n",
       "...                  ...               ...          ...           ...  ...   \n",
       "2144            1.561126          4.049964     6.555306      7.535540  ...   \n",
       "2145           18.767261          1.360667     2.904662      8.555256  ...   \n",
       "2146            4.594670          9.886002     8.120025      5.769464  ...   \n",
       "2147            8.674505          6.354282     1.263427      8.322874  ...   \n",
       "2148            7.890703          6.570993     7.941404      9.878711  ...   \n",
       "\n",
       "      FunctionalAssessment  MemoryComplaints  BehavioralProblems       ADL  \\\n",
       "0                 6.518877                 0                   0  1.725883   \n",
       "1                 7.118696                 0                   0  2.592424   \n",
       "2                 5.895077                 0                   0  7.119548   \n",
       "3                 8.965106                 0                   1  6.481226   \n",
       "4                 6.045039                 0                   0  0.014691   \n",
       "...                    ...               ...                 ...       ...   \n",
       "2144              0.238667                 0                   0  4.492838   \n",
       "2145              8.687480                 0                   1  9.204952   \n",
       "2146              1.972137                 0                   0  5.036334   \n",
       "2147              5.173891                 0                   0  3.785399   \n",
       "2148              6.307543                 0                   1  8.327563   \n",
       "\n",
       "      Confusion  Disorientation  PersonalityChanges  \\\n",
       "0             0               0                   0   \n",
       "1             0               0                   0   \n",
       "2             0               1                   0   \n",
       "3             0               0                   0   \n",
       "4             0               0                   1   \n",
       "...         ...             ...                 ...   \n",
       "2144          1               0                   0   \n",
       "2145          0               0                   0   \n",
       "2146          0               0                   0   \n",
       "2147          0               0                   0   \n",
       "2148          0               1                   0   \n",
       "\n",
       "      DifficultyCompletingTasks  Forgetfulness  Diagnosis  \n",
       "0                             1              0          0  \n",
       "1                             0              1          0  \n",
       "2                             1              0          0  \n",
       "3                             0              0          0  \n",
       "4                             1              0          0  \n",
       "...                         ...            ...        ...  \n",
       "2144                          0              0          1  \n",
       "2145                          0              0          1  \n",
       "2146                          0              0          1  \n",
       "2147                          0              1          1  \n",
       "2148                          0              1          0  \n",
       "\n",
       "[2149 rows x 33 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing logistic_regression_standard...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.50904675        nan        nan 0.51730756 0.51730756\n",
      "        nan 0.50904675        nan        nan 0.51730756 0.51730756\n",
      "        nan 0.50904675        nan        nan 0.51730756 0.51730756\n",
      "        nan 0.52305876        nan        nan 0.5140255  0.51484855\n",
      "        nan 0.52305876        nan        nan 0.5140255  0.51484855\n",
      "        nan 0.52305876        nan        nan 0.5140255  0.51484855\n",
      "        nan 0.51813398        nan        nan 0.51320246 0.51320246\n",
      "        nan 0.51813398        nan        nan 0.51320246 0.51320246\n",
      "        nan 0.51813398        nan        nan 0.51320246 0.51320246\n",
      "        nan 0.5140255         nan        nan 0.51320246 0.51320246\n",
      "        nan 0.5140255         nan        nan 0.51320246 0.51320246\n",
      "        nan 0.5140255         nan        nan 0.51320246 0.51320246\n",
      "        nan 0.51320246        nan        nan 0.51320246 0.51320246\n",
      "        nan 0.51320246        nan        nan 0.51320246 0.51320246\n",
      "        nan 0.51320246        nan        nan 0.51320246 0.51320246]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for logistic_regression_standard: {'classifier__C': 0.1, 'classifier__max_iter': 100, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
      "Best score for logistic_regression_standard: 0.5230587600350807\n",
      "Optimizing logistic_regression_minmax...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.50493153        nan        nan 0.50004048 0.49921743\n",
      "        nan 0.50904675        nan        nan 0.50004048 0.49921743\n",
      "        nan 0.50329893        nan        nan 0.50004048 0.49921743\n",
      "        nan 0.5172772         nan        nan 0.51811374 0.51811037\n",
      "        nan 0.5172772         nan        nan 0.51811374 0.51811037\n",
      "        nan 0.51645416        nan        nan 0.51811374 0.51811037\n",
      "        nan 0.52388518        nan        nan 0.51813061 0.51813061\n",
      "        nan 0.52388518        nan        nan 0.51813061 0.51813061\n",
      "        nan 0.52388518        nan        nan 0.51813061 0.51813061\n",
      "        nan 0.51648452        nan        nan 0.5140255  0.51320246\n",
      "        nan 0.51648452        nan        nan 0.5140255  0.51320246\n",
      "        nan 0.51648452        nan        nan 0.5140255  0.51320246\n",
      "        nan 0.51320246        nan        nan 0.5140255  0.51320246\n",
      "        nan 0.51320246        nan        nan 0.5140255  0.51320246\n",
      "        nan 0.51320246        nan        nan 0.5140255  0.51320246]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for logistic_regression_minmax: {'classifier__C': 1, 'classifier__max_iter': 100, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
      "Best score for logistic_regression_minmax: 0.5238851784389126\n",
      "Optimizing decision_tree...\n",
      "Best parameters for decision_tree: {'classifier__criterion': 'entropy', 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 10}\n",
      "Best score for decision_tree: 0.5378297240774472\n",
      "Optimizing random_forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for random_forest: {'classifier__bootstrap': True, 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 5, 'classifier__min_samples_split': 20, 'classifier__n_estimators': 200}\n",
      "Best score for random_forest: 0.5353707076840045\n",
      "Optimizing gradient_boosting...\n",
      "Best parameters for gradient_boosting: {'classifier__learning_rate': 0.001, 'classifier__max_depth': 3, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
      "Best score for gradient_boosting: 0.5411117857383795\n",
      "Optimizing svm_standard...\n",
      "Best parameters for svm_standard: {'classifier__C': 1, 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear'}\n",
      "Best score for svm_standard: 0.5354010659110842\n",
      "Optimizing svm_minmax...\n",
      "Best parameters for svm_minmax: {'classifier__C': 100, 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear'}\n",
      "Best score for svm_minmax: 0.5354010659110842\n",
      "Optimizing knn_standard...\n",
      "Best parameters for knn_standard: {'classifier__n_neighbors': 3, 'classifier__p': 2, 'classifier__weights': 'distance'}\n",
      "Best score for knn_standard: 0.51724684611752\n",
      "Optimizing knn_minmax...\n",
      "Best parameters for knn_minmax: {'classifier__n_neighbors': 3, 'classifier__p': 2, 'classifier__weights': 'distance'}\n",
      "Best score for knn_minmax: 0.5164170545773461\n",
      "Optimizing naive_bayes_standard...\n",
      "Best parameters for naive_bayes_standard: {'classifier__var_smoothing': 1e-10}\n",
      "Best score for naive_bayes_standard: 0.5066147203669973\n",
      "Optimizing naive_bayes_minmax...\n",
      "Best parameters for naive_bayes_minmax: {'classifier__var_smoothing': 1e-10}\n",
      "Best score for naive_bayes_minmax: 0.5066147203669973\n",
      "\n",
      "Model: logistic_regression_standard\n",
      "Accuracy: 0.4769736842105263\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.39      0.44       163\n",
      "           1       0.45      0.58      0.51       141\n",
      "\n",
      "    accuracy                           0.48       304\n",
      "   macro avg       0.48      0.48      0.47       304\n",
      "weighted avg       0.49      0.48      0.47       304\n",
      "\n",
      "\n",
      "Model: logistic_regression_minmax\n",
      "Accuracy: 0.4901315789473684\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.41      0.46       163\n",
      "           1       0.46      0.58      0.51       141\n",
      "\n",
      "    accuracy                           0.49       304\n",
      "   macro avg       0.50      0.50      0.49       304\n",
      "weighted avg       0.50      0.49      0.49       304\n",
      "\n",
      "\n",
      "Model: decision_tree\n",
      "Accuracy: 0.46710526315789475\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.39      0.44       163\n",
      "           1       0.44      0.55      0.49       141\n",
      "\n",
      "    accuracy                           0.47       304\n",
      "   macro avg       0.47      0.47      0.47       304\n",
      "weighted avg       0.47      0.47      0.46       304\n",
      "\n",
      "\n",
      "Model: random_forest\n",
      "Accuracy: 0.5032894736842105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.37      0.45       163\n",
      "           1       0.47      0.65      0.55       141\n",
      "\n",
      "    accuracy                           0.50       304\n",
      "   macro avg       0.51      0.51      0.50       304\n",
      "weighted avg       0.52      0.50      0.49       304\n",
      "\n",
      "\n",
      "Model: gradient_boosting\n",
      "Accuracy: 0.46381578947368424\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.10      0.17       163\n",
      "           1       0.46      0.88      0.60       141\n",
      "\n",
      "    accuracy                           0.46       304\n",
      "   macro avg       0.48      0.49      0.39       304\n",
      "weighted avg       0.48      0.46      0.37       304\n",
      "\n",
      "\n",
      "Model: svm_standard\n",
      "Accuracy: 0.4901315789473684\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.45      0.49       163\n",
      "           1       0.46      0.53      0.49       141\n",
      "\n",
      "    accuracy                           0.49       304\n",
      "   macro avg       0.49      0.49      0.49       304\n",
      "weighted avg       0.50      0.49      0.49       304\n",
      "\n",
      "\n",
      "Model: svm_minmax\n",
      "Accuracy: 0.4901315789473684\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.45      0.49       163\n",
      "           1       0.46      0.53      0.49       141\n",
      "\n",
      "    accuracy                           0.49       304\n",
      "   macro avg       0.49      0.49      0.49       304\n",
      "weighted avg       0.50      0.49      0.49       304\n",
      "\n",
      "\n",
      "Model: knn_standard\n",
      "Accuracy: 0.4769736842105263\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.52      0.51       163\n",
      "           1       0.44      0.43      0.43       141\n",
      "\n",
      "    accuracy                           0.48       304\n",
      "   macro avg       0.47      0.47      0.47       304\n",
      "weighted avg       0.48      0.48      0.48       304\n",
      "\n",
      "\n",
      "Model: knn_minmax\n",
      "Accuracy: 0.46381578947368424\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.49      0.50       163\n",
      "           1       0.42      0.43      0.43       141\n",
      "\n",
      "    accuracy                           0.46       304\n",
      "   macro avg       0.46      0.46      0.46       304\n",
      "weighted avg       0.46      0.46      0.46       304\n",
      "\n",
      "\n",
      "Model: naive_bayes_standard\n",
      "Accuracy: 0.5032894736842105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.42      0.47       163\n",
      "           1       0.47      0.60      0.53       141\n",
      "\n",
      "    accuracy                           0.50       304\n",
      "   macro avg       0.51      0.51      0.50       304\n",
      "weighted avg       0.51      0.50      0.50       304\n",
      "\n",
      "\n",
      "Model: naive_bayes_minmax\n",
      "Accuracy: 0.5032894736842105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.42      0.47       163\n",
      "           1       0.47      0.60      0.53       141\n",
      "\n",
      "    accuracy                           0.50       304\n",
      "   macro avg       0.51      0.51      0.50       304\n",
      "weighted avg       0.51      0.50      0.50       304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_question, X_test_question, y_train_question, y_test_question = train_test_split(X[['Age', 'Gender', 'Ethnicity', 'EducationLevel', 'BMI', 'Smoking', 'AlcoholConsumption', 'PhysicalActivity', 'DietQuality', 'SleepQuality']], y, test_size=0.2, random_state=0)\n",
    "generetate_models(X_train_question, y_train_question, X_test_question, y_test_question, 'questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "données soumises [{'Age': 76, 'Gender': 1, 'Ethnicity': 2, 'EducationLevel': 0, 'BMI': 33.8293639108859, 'Smoking': 0, 'AlcoholConsumption': 19.82631309137609, 'PhysicalActivity': 9.399462708710182, 'DietQuality': 4.794106212456812, 'SleepQuality': 4.976045740143789}]\n",
      "réponse attendue 1\n",
      "réponse prédicte [0.45739536 0.54260464]\n",
      "Prédiction: [0.45739536 0.54260464], Confiance: 54.26%\n"
     ]
    }
   ],
   "source": [
    "predict('questions/random_forest.joblib', X_test_question, y_test_question, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing logistic_regression_standard...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.50493153        nan        nan 0.51891992 0.51891992\n",
      "        nan 0.50904675        nan        nan 0.51891992 0.51891992\n",
      "        nan 0.50904675        nan        nan 0.51891992 0.51891992\n",
      "        nan 0.51645753        nan        nan 0.51563111 0.51480807\n",
      "        nan 0.51645753        nan        nan 0.51563111 0.51480807\n",
      "        nan 0.51645753        nan        nan 0.51563111 0.51480807\n",
      "        nan 0.51316198        nan        nan 0.51480807 0.51480807\n",
      "        nan 0.51316198        nan        nan 0.51480807 0.51480807\n",
      "        nan 0.51316198        nan        nan 0.51480807 0.51480807\n",
      "        nan 0.51480807        nan        nan 0.51480807 0.51480807\n",
      "        nan 0.51480807        nan        nan 0.51480807 0.51480807\n",
      "        nan 0.51480807        nan        nan 0.51480807 0.51480807\n",
      "        nan 0.51480807        nan        nan 0.51480807 0.51480807\n",
      "        nan 0.51480807        nan        nan 0.51480807 0.51480807\n",
      "        nan 0.51480807        nan        nan 0.51480807 0.51480807]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for logistic_regression_standard: {'classifier__C': 0.01, 'classifier__max_iter': 100, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
      "Best score for logistic_regression_standard: 0.5189199217432369\n",
      "Optimizing logistic_regression_minmax...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.50904675        nan        nan 0.5172772  0.51809688\n",
      "        nan 0.50904675        nan        nan 0.5172772  0.51809688\n",
      "        nan 0.50246239        nan        nan 0.5172772  0.51809688\n",
      "        nan 0.50904675        nan        nan 0.51480807 0.51480807\n",
      "        nan 0.50904675        nan        nan 0.51480807 0.51480807\n",
      "        nan 0.50904675        nan        nan 0.51480807 0.51480807\n",
      "        nan 0.51151926        nan        nan 0.51398502 0.51398502\n",
      "        nan 0.51151926        nan        nan 0.51398502 0.51398502\n",
      "        nan 0.51151926        nan        nan 0.51398502 0.51398502\n",
      "        nan 0.51480807        nan        nan 0.51398502 0.51480807\n",
      "        nan 0.51398502        nan        nan 0.51398502 0.51480807\n",
      "        nan 0.51480807        nan        nan 0.51398502 0.51480807\n",
      "        nan 0.51480807        nan        nan 0.51398502 0.51480807\n",
      "        nan 0.51480807        nan        nan 0.51398502 0.51480807\n",
      "        nan 0.51480807        nan        nan 0.51398502 0.51480807]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for logistic_regression_minmax: {'classifier__C': 0.01, 'classifier__max_iter': 100, 'classifier__penalty': 'l2', 'classifier__solver': 'saga'}\n",
      "Best score for logistic_regression_minmax: 0.5180968764757472\n",
      "Optimizing decision_tree...\n",
      "Best parameters for decision_tree: {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2}\n",
      "Best score for decision_tree: 0.5345679012345679\n",
      "Optimizing random_forest...\n",
      "Best parameters for random_forest: {'classifier__bootstrap': False, 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 50}\n",
      "Best score for random_forest: 0.5337246171490252\n",
      "Optimizing gradient_boosting...\n",
      "Best parameters for gradient_boosting: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 10, 'classifier__n_estimators': 200, 'classifier__subsample': 0.8}\n",
      "Best score for gradient_boosting: 0.543628145449639\n",
      "Optimizing svm_standard...\n",
      "Best parameters for svm_standard: {'classifier__C': 10, 'classifier__degree': 4, 'classifier__gamma': 'scale', 'classifier__kernel': 'poly'}\n",
      "Best score for svm_standard: 0.5189232948795791\n",
      "Optimizing svm_minmax...\n",
      "Best parameters for svm_minmax: {'classifier__C': 0.1, 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear'}\n",
      "Best score for svm_minmax: 0.5181069958847736\n",
      "Optimizing knn_standard...\n",
      "Best parameters for knn_standard: {'classifier__n_neighbors': 3, 'classifier__p': 1, 'classifier__weights': 'uniform'}\n",
      "Best score for knn_standard: 0.5131383660527558\n",
      "Optimizing knn_minmax...\n",
      "Best parameters for knn_minmax: {'classifier__n_neighbors': 5, 'classifier__p': 1, 'classifier__weights': 'uniform'}\n",
      "Best score for knn_minmax: 0.511485529245092\n",
      "Optimizing naive_bayes_standard...\n",
      "Best parameters for naive_bayes_standard: {'classifier__var_smoothing': 1e-10}\n",
      "Best score for naive_bayes_standard: 0.5172502192538622\n",
      "Optimizing naive_bayes_minmax...\n",
      "Best parameters for naive_bayes_minmax: {'classifier__var_smoothing': 1e-10}\n",
      "Best score for naive_bayes_minmax: 0.5172502192538622\n",
      "\n",
      "Model: logistic_regression_standard\n",
      "Accuracy: 0.5328947368421053\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.50      0.54       163\n",
      "           1       0.50      0.57      0.53       141\n",
      "\n",
      "    accuracy                           0.53       304\n",
      "   macro avg       0.54      0.54      0.53       304\n",
      "weighted avg       0.54      0.53      0.53       304\n",
      "\n",
      "\n",
      "Model: logistic_regression_minmax\n",
      "Accuracy: 0.5032894736842105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.30      0.39       163\n",
      "           1       0.48      0.74      0.58       141\n",
      "\n",
      "    accuracy                           0.50       304\n",
      "   macro avg       0.52      0.52      0.49       304\n",
      "weighted avg       0.53      0.50      0.48       304\n",
      "\n",
      "\n",
      "Model: decision_tree\n",
      "Accuracy: 0.5361842105263158\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.48      0.53       163\n",
      "           1       0.50      0.60      0.54       141\n",
      "\n",
      "    accuracy                           0.54       304\n",
      "   macro avg       0.54      0.54      0.54       304\n",
      "weighted avg       0.54      0.54      0.54       304\n",
      "\n",
      "\n",
      "Model: random_forest\n",
      "Accuracy: 0.5296052631578947\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.45      0.51       163\n",
      "           1       0.49      0.62      0.55       141\n",
      "\n",
      "    accuracy                           0.53       304\n",
      "   macro avg       0.54      0.54      0.53       304\n",
      "weighted avg       0.54      0.53      0.53       304\n",
      "\n",
      "\n",
      "Model: gradient_boosting\n",
      "Accuracy: 0.4868421052631579\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.48      0.50       163\n",
      "           1       0.45      0.50      0.47       141\n",
      "\n",
      "    accuracy                           0.49       304\n",
      "   macro avg       0.49      0.49      0.49       304\n",
      "weighted avg       0.49      0.49      0.49       304\n",
      "\n",
      "\n",
      "Model: svm_standard\n",
      "Accuracy: 0.45394736842105265\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.55      0.52       163\n",
      "           1       0.40      0.35      0.37       141\n",
      "\n",
      "    accuracy                           0.45       304\n",
      "   macro avg       0.45      0.45      0.44       304\n",
      "weighted avg       0.45      0.45      0.45       304\n",
      "\n",
      "\n",
      "Model: svm_minmax\n",
      "Accuracy: 0.5526315789473685\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.56      0.57       163\n",
      "           1       0.52      0.54      0.53       141\n",
      "\n",
      "    accuracy                           0.55       304\n",
      "   macro avg       0.55      0.55      0.55       304\n",
      "weighted avg       0.55      0.55      0.55       304\n",
      "\n",
      "\n",
      "Model: knn_standard\n",
      "Accuracy: 0.5296052631578947\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.56      0.56       163\n",
      "           1       0.49      0.50      0.49       141\n",
      "\n",
      "    accuracy                           0.53       304\n",
      "   macro avg       0.53      0.53      0.53       304\n",
      "weighted avg       0.53      0.53      0.53       304\n",
      "\n",
      "\n",
      "Model: knn_minmax\n",
      "Accuracy: 0.5296052631578947\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.57      0.57       163\n",
      "           1       0.49      0.48      0.49       141\n",
      "\n",
      "    accuracy                           0.53       304\n",
      "   macro avg       0.53      0.53      0.53       304\n",
      "weighted avg       0.53      0.53      0.53       304\n",
      "\n",
      "\n",
      "Model: naive_bayes_standard\n",
      "Accuracy: 0.5493421052631579\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.63      0.60       163\n",
      "           1       0.52      0.46      0.49       141\n",
      "\n",
      "    accuracy                           0.55       304\n",
      "   macro avg       0.54      0.54      0.54       304\n",
      "weighted avg       0.55      0.55      0.55       304\n",
      "\n",
      "\n",
      "Model: naive_bayes_minmax\n",
      "Accuracy: 0.5493421052631579\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.63      0.60       163\n",
      "           1       0.52      0.46      0.49       141\n",
      "\n",
      "    accuracy                           0.55       304\n",
      "   macro avg       0.54      0.54      0.54       304\n",
      "weighted avg       0.55      0.55      0.55       304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_clinical, X_test_clinical, y_train_clinical, y_test_clinical = train_test_split(X[['FamilyHistoryAlzheimers', 'CardiovascularDisease', 'Diabetes', 'Depression', 'HeadInjury', 'Hypertension', 'SystolicBP', 'DiastolicBP', 'CholesterolTotal', 'CholesterolLDL', 'CholesterolHDL', 'CholesterolTriglycerides']], y, test_size=0.2, random_state=0)\n",
    "generetate_models(X_train_clinical, y_train_clinical, X_test_clinical, y_test_clinical, 'clinical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "données soumises [{'FamilyHistoryAlzheimers': 0, 'CardiovascularDisease': 1, 'Diabetes': 1, 'Depression': 1, 'HeadInjury': 0, 'Hypertension': 0, 'SystolicBP': 125, 'DiastolicBP': 108, 'CholesterolTotal': 270.5244657436494, 'CholesterolLDL': 113.19711983229844, 'CholesterolHDL': 35.326907803137615, 'CholesterolTriglycerides': 152.87039903395123}]\n",
      "réponse attendue 1\n",
      "réponse prédicte [0.47541676 0.52458324]\n",
      "Prédiction: [0.47541676 0.52458324], Confiance: 52.46%\n"
     ]
    }
   ],
   "source": [
    "predict('clinical/random_forest.joblib', X_test_clinical, y_test_clinical, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing logistic_regression_standard...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.79112191        nan        nan 0.82894151 0.82976456\n",
      "        nan 0.79112191        nan        nan 0.82894151 0.82976456\n",
      "        nan 0.79112191        nan        nan 0.82894151 0.82976456\n",
      "        nan 0.83059435        nan        nan 0.82894151 0.82894151\n",
      "        nan 0.83059435        nan        nan 0.82894151 0.82894151\n",
      "        nan 0.83059435        nan        nan 0.82894151 0.82894151\n",
      "        nan 0.82894488        nan        nan 0.82729879 0.82729879\n",
      "        nan 0.82894488        nan        nan 0.82729879 0.82729879\n",
      "        nan 0.82894488        nan        nan 0.82729879 0.82729879\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82812184\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82729879\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82729879\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82812184\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82812184\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82812184]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for logistic_regression_standard: {'classifier__C': 0.1, 'classifier__max_iter': 100, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
      "Best score for logistic_regression_standard: 0.8305943466234906\n",
      "Optimizing logistic_regression_minmax...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.50493153        nan        nan 0.81989476 0.81989476\n",
      "        nan 0.50904675        nan        nan 0.81989476 0.81989476\n",
      "        nan 0.50493153        nan        nan 0.81989476 0.81989476\n",
      "        nan 0.8363388         nan        nan 0.83057411 0.83057411\n",
      "        nan 0.8363388         nan        nan 0.83057411 0.83057411\n",
      "        nan 0.8363388         nan        nan 0.83057411 0.83057411\n",
      "        nan 0.83059097        nan        nan 0.82811846 0.82729879\n",
      "        nan 0.83059097        nan        nan 0.82811846 0.82729879\n",
      "        nan 0.83059097        nan        nan 0.82811846 0.82729879\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82812184\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82812184\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82812184\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82812184\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82812184\n",
      "        nan 0.82812184        nan        nan 0.82812184 0.82812184]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for logistic_regression_minmax: {'classifier__C': 0.1, 'classifier__max_iter': 100, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
      "Best score for logistic_regression_minmax: 0.8363387978142077\n",
      "Optimizing decision_tree...\n",
      "Best parameters for decision_tree: {'classifier__criterion': 'entropy', 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 20}\n",
      "Best score for decision_tree: 0.9366896039937934\n",
      "Optimizing random_forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for random_forest: {'classifier__bootstrap': True, 'classifier__max_depth': 10, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 100}\n",
      "Best score for random_forest: 0.9465594009309857\n",
      "Optimizing gradient_boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\Desktop\\projets-web\\alzheimer_form\\.venv\\lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for gradient_boosting: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__n_estimators': 100, 'classifier__subsample': 0.8}\n",
      "Best score for gradient_boosting: 0.9391553666599204\n",
      "Optimizing svm_standard...\n",
      "Best parameters for svm_standard: {'classifier__C': 10, 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'rbf'}\n",
      "Best score for svm_standard: 0.871726371179923\n",
      "Optimizing svm_minmax...\n",
      "Best parameters for svm_minmax: {'classifier__C': 10, 'classifier__degree': 3, 'classifier__gamma': 'scale', 'classifier__kernel': 'rbf'}\n",
      "Best score for svm_minmax: 0.8667982189840113\n",
      "Optimizing knn_standard...\n",
      "Best parameters for knn_standard: {'classifier__n_neighbors': 5, 'classifier__p': 1, 'classifier__weights': 'uniform'}\n",
      "Best score for knn_standard: 0.8569149295014504\n",
      "Optimizing knn_minmax...\n",
      "Best parameters for knn_minmax: {'classifier__n_neighbors': 9, 'classifier__p': 2, 'classifier__weights': 'distance'}\n",
      "Best score for knn_minmax: 0.8519867773055386\n",
      "Optimizing naive_bayes_standard...\n",
      "Best parameters for naive_bayes_standard: {'classifier__var_smoothing': 1e-10}\n",
      "Best score for naive_bayes_standard: 0.8182587870201713\n",
      "Optimizing naive_bayes_minmax...\n",
      "Best parameters for naive_bayes_minmax: {'classifier__var_smoothing': 1e-10}\n",
      "Best score for naive_bayes_minmax: 0.8182587870201713\n",
      "\n",
      "Model: logistic_regression_standard\n",
      "Accuracy: 0.8157894736842105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       163\n",
      "           1       0.80      0.80      0.80       141\n",
      "\n",
      "    accuracy                           0.82       304\n",
      "   macro avg       0.81      0.81      0.81       304\n",
      "weighted avg       0.82      0.82      0.82       304\n",
      "\n",
      "\n",
      "Model: logistic_regression_minmax\n",
      "Accuracy: 0.8157894736842105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83       163\n",
      "           1       0.80      0.81      0.80       141\n",
      "\n",
      "    accuracy                           0.82       304\n",
      "   macro avg       0.81      0.82      0.81       304\n",
      "weighted avg       0.82      0.82      0.82       304\n",
      "\n",
      "\n",
      "Model: decision_tree\n",
      "Accuracy: 0.930921052631579\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94       163\n",
      "           1       0.94      0.91      0.92       141\n",
      "\n",
      "    accuracy                           0.93       304\n",
      "   macro avg       0.93      0.93      0.93       304\n",
      "weighted avg       0.93      0.93      0.93       304\n",
      "\n",
      "\n",
      "Model: random_forest\n",
      "Accuracy: 0.9375\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       163\n",
      "           1       0.96      0.91      0.93       141\n",
      "\n",
      "    accuracy                           0.94       304\n",
      "   macro avg       0.94      0.94      0.94       304\n",
      "weighted avg       0.94      0.94      0.94       304\n",
      "\n",
      "\n",
      "Model: gradient_boosting\n",
      "Accuracy: 0.9342105263157895\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       163\n",
      "           1       0.94      0.91      0.93       141\n",
      "\n",
      "    accuracy                           0.93       304\n",
      "   macro avg       0.93      0.93      0.93       304\n",
      "weighted avg       0.93      0.93      0.93       304\n",
      "\n",
      "\n",
      "Model: svm_standard\n",
      "Accuracy: 0.881578947368421\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89       163\n",
      "           1       0.89      0.84      0.87       141\n",
      "\n",
      "    accuracy                           0.88       304\n",
      "   macro avg       0.88      0.88      0.88       304\n",
      "weighted avg       0.88      0.88      0.88       304\n",
      "\n",
      "\n",
      "Model: svm_minmax\n",
      "Accuracy: 0.8782894736842105\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89       163\n",
      "           1       0.89      0.84      0.86       141\n",
      "\n",
      "    accuracy                           0.88       304\n",
      "   macro avg       0.88      0.88      0.88       304\n",
      "weighted avg       0.88      0.88      0.88       304\n",
      "\n",
      "\n",
      "Model: knn_standard\n",
      "Accuracy: 0.8453947368421053\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85       163\n",
      "           1       0.81      0.87      0.84       141\n",
      "\n",
      "    accuracy                           0.85       304\n",
      "   macro avg       0.84      0.85      0.85       304\n",
      "weighted avg       0.85      0.85      0.85       304\n",
      "\n",
      "\n",
      "Model: knn_minmax\n",
      "Accuracy: 0.8717105263157895\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.88       163\n",
      "           1       0.84      0.89      0.87       141\n",
      "\n",
      "    accuracy                           0.87       304\n",
      "   macro avg       0.87      0.87      0.87       304\n",
      "weighted avg       0.87      0.87      0.87       304\n",
      "\n",
      "\n",
      "Model: naive_bayes_standard\n",
      "Accuracy: 0.8026315789473685\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.79      0.81       163\n",
      "           1       0.77      0.82      0.79       141\n",
      "\n",
      "    accuracy                           0.80       304\n",
      "   macro avg       0.80      0.80      0.80       304\n",
      "weighted avg       0.80      0.80      0.80       304\n",
      "\n",
      "\n",
      "Model: naive_bayes_minmax\n",
      "Accuracy: 0.8026315789473685\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.79      0.81       163\n",
      "           1       0.77      0.82      0.79       141\n",
      "\n",
      "    accuracy                           0.80       304\n",
      "   macro avg       0.80      0.80      0.80       304\n",
      "weighted avg       0.80      0.80      0.80       304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_cognitive, X_test_cognitive, y_train_cognitive, y_test_cognitive = train_test_split(X[['MMSE', 'FunctionalAssessment', 'MemoryComplaints', 'BehavioralProblems', 'ADL', 'Confusion', 'Disorientation', 'DifficultyCompletingTasks', 'Forgetfulness']], y, test_size=0.2, random_state=0)\n",
    "generetate_models(X_train_cognitive, y_train_cognitive, X_test_cognitive, y_test_cognitive, 'cognitive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "données soumises [{'MMSE': 26.221478092482634, 'FunctionalAssessment': 3.6758191861947633, 'MemoryComplaints': 0, 'BehavioralProblems': 1, 'ADL': 5.565379424980053, 'Confusion': 0, 'Disorientation': 0, 'DifficultyCompletingTasks': 0, 'Forgetfulness': 0}]\n",
      "réponse attendue 0\n",
      "réponse prédicte [0.87607906 0.12392094]\n",
      "Prédiction: [0.87607906 0.12392094], Confiance: 87.61%\n"
     ]
    }
   ],
   "source": [
    "predict('cognitive/random_forest.joblib', X_test_cognitive, y_test_cognitive, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tpot import TPOTClassifier\n",
    "\n",
    "# y = df.Diagnosis\n",
    "# X = df.drop(columns=['Diagnosis'])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# tpot = TPOTClassifier(generations=5, population_size=20, verbosity=3)\n",
    "# tpot.fit(X_train, y_train)\n",
    "# print(tpot.score(X_test, y_test))\n",
    "# tpot.export('tpot_best_pipeline.py')  # Génère le code Python optimisé\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict('decision_tree.joblib', X_test, y_test, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
